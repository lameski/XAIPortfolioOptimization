{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486f578b",
   "metadata": {},
   "source": [
    "### Case study of XAI portfolio building and optimization\n",
    "\n",
    "This notebook gives an overview of the steps taken for portfolio optimization using AI and leveraging XAI. The portfolio optimization notebook works with openly available datasets, but any other data can be used as long as its compatible. The steps for the portfolio building and optimizations are as follows:\n",
    "\n",
    "        1. Install and import necessary libraries\n",
    "        2. Declare constans and vairables\n",
    "        3. Setup dataset\n",
    "        4. Setup goals (Portfolio size and expected performance)\n",
    "        5. Construct portfolio \n",
    "        6. Masure performance\n",
    "        7. Using explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5c3f8",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b23b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import shap\n",
    "import lime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74de43f",
   "metadata": {},
   "source": [
    "#### Constants and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE  = datetime.strptime(\"2010-01-01\", \"%Y-%m-%d\").date()\n",
    "SPLIT_DATE  = datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\").date()\n",
    "END_DATE    = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").date()\n",
    "LABEL       = \"Close\"\n",
    "WINDOW      = 50\n",
    "\n",
    "def create_dataset(dataset, window):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-window-1):\n",
    "        a = dataset[i:(i+window), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + window, :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "def create_portfolios(df,num_portfolios,mean_daily_returns,cov_matrix):\n",
    "    num_symbols = len(df['Symbol'].unique())\n",
    "    results = np.zeros((3 + num_symbols, num_portfolios))\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.array(np.random.random(num_symbols))\n",
    "        weights /= np.sum(weights)\n",
    "        portfolio_return = np.sum(mean_daily_returns * weights) * 252\n",
    "        portfolio_std_dev = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "        results[0, i] = portfolio_return\n",
    "        results[1, i] = portfolio_std_dev\n",
    "        # Store Sharpe Ratio (return / volatility) - risk free rate element excluded for simplicity\n",
    "        results[2, i] = results[0, i] / results[1, i]\n",
    "        for j in range(len(weights)):\n",
    "            results[j+3, i] = weights[j]\n",
    "    results_frame = pd.DataFrame(results.T, columns=['return', 'stdev', 'sharpe'] + [str(i) for i in range(num_symbols)])\n",
    "    return results_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c68bc",
   "metadata": {},
   "source": [
    "#### Download stocks and assets\n",
    "\n",
    "In this notebook we use publicly available datasets. One from www.kaggle.com for SP500 stocks (https://www.kaggle.com/datasets/andrewmvd/sp-500-stocks) and additionally yahoo finance (https://pypi.org/project/yfinance/) for other assets.\n",
    "The dataset SP500 needs to be downloaded from the source first.\n",
    "The dataset with commodities is downloaded from yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "\n",
    "#select 10 stocks from sp500 to reduce dataset for faster execution of this notebook\n",
    "\n",
    "selected = ['AAPL','MSFT','AMZN','NVDA','GOOGL','TSLA','JPM','WMT','PG','PFE']\n",
    "dateparse = lambda dates: datetime.strptime(dates, '%Y-%m-%d')\n",
    "iter_csv = pd.read_csv(path.join(os.getcwd(),'sp500_stocks.csv'), iterator=True, chunksize=1000, parse_dates = ['Date'], index_col = 'Date', date_parser = dateparse)\n",
    "df = pd.concat([chunk[chunk['Symbol'].isin(selected)] for chunk in iter_csv])\n",
    "\n",
    "\n",
    "\n",
    "# Downloading gold, oil, natural gas, currencies, Bitcoin\n",
    "other = ['GC=F','CL=F','NG=F','GBPUSD=X','EURUSD=X','BTC-USD']\n",
    "all = []\n",
    "for c in other:\n",
    "    cmd = yf.download(c, start=START_DATE, end=END_DATE, interval='1d')\n",
    "    cmd['Symbol'] = c\n",
    "    all.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "dfOther = pd.concat(all)\n",
    "df = pd.concat([df, dfOther])\n",
    "df = df.sort_values('Date')\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "df = df.reset_index()\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table per symbol\n",
    "pivot_df = df.pivot(columns='Symbol', values=LABEL)\n",
    "# Daily returns\n",
    "returns = pivot_df.pct_change()\n",
    "# Mean returns and the covariance matrix of returns\n",
    "mean_daily_returns = returns.mean()\n",
    "cov_matrix = returns.cov()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfa5f9",
   "metadata": {},
   "source": [
    "#### Portfolio selection using random forest and explaining using SHAP\n",
    "\n",
    "We chose a number of portfolios to be randomly selected and allocated from the data\n",
    "After this we calculate the returns and volatility of each portfolio\n",
    "Then we use ML approach (random forest) to predict the sharpe value of each portfolio\n",
    "In this way we can select the best portfolio by using sharpe value as metric.\n",
    "Finally we select the 10 best performing portfolios and use LIME to explain each of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7bfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_portfolios = 10000\n",
    "\n",
    "results_frame = create_portfolios(pivot_df,num_portfolios,mean_daily_returns,cov_matrix)\n",
    "\n",
    "# Define dataset for ML\n",
    "X = results_frame.drop('sharpe', axis=1)\n",
    "y = results_frame['sharpe']\n",
    "\n",
    "# Fit model using RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "#Explain using SHAP\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top portfolios:\n",
    "\n",
    "# Locate position of portfolio with highest Sharpe Ratio\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "# Locate positon of portfolio with minimum standard deviation\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "# Compute SHAP values for top 10 portfolios\n",
    "shap_values = explainer.shap_values([max_sharpe_port,min_vol_port])\n",
    "\n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, [max_sharpe_port,min_vol_port], plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LimeTabularExplainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X.values, feature_names=X.columns, class_names=['sharpe'], verbose=True, mode='regression')\n",
    "\n",
    "# Explain each of the top 10 portfolios\n",
    "for i in range(len(top_portfolios)):\n",
    "    exp = explainer.explain_instance(top_portfolios.iloc[i], model.predict, num_features=5)\n",
    "    print('Portfolio', i)\n",
    "    exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c80356",
   "metadata": {},
   "source": [
    "## Portfolio selection using LSTM and XAI\n",
    "\n",
    "We use LSTM to predict returns and use XAI to explain the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess again\n",
    "# Pivot table per symbol\n",
    "pivot_df = df.pivot(columns='Symbol', values=LABEL)\n",
    "\n",
    "# Daily returns\n",
    "returns = pivot_df.pct_change()\n",
    "returns = returns.dropna()\n",
    "# Scale data to fit for NN approach\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58231535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train and test set \n",
    "split_ratio = 0.9\n",
    "train_data = scaled_data[:int(split_ratio*len(scaled_data))]\n",
    "test_data = scaled_data[int(split_ratio*len(scaled_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create training and test datasets\n",
    "trainX, trainY = create_dataset(train_data, WINDOW)\n",
    "testX, testY = create_dataset(test_data, WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b86dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db26aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(testY[0], color='blue', label='Actual return')\n",
    "plt.plot(testPredict[:,0], color='red', label='Predicted return')\n",
    "plt.title('Return Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7afda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explain model\n",
    "\n",
    "# Create a LimeTabularExplainer\n",
    "explainer = lime_tabular.LimeTabularExplainer(trainX.reshape(trainX.shape[0], -1), \n",
    "                                              feature_names = [f\"Day {i+1}\" for i in range(trainX.shape[1])], \n",
    "                                              class_names=['Return'], \n",
    "                                              verbose=True, \n",
    "                                              mode='regression')\n",
    "\n",
    "# Explain a prediction (explain single instance of prediction)\n",
    "exp = explainer.explain_instance(testX[0].reshape(-1), model.predict, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521335fb",
   "metadata": {},
   "source": [
    "### Using LSTM predicted returns to optimize portfolio\n",
    "\n",
    "This time we use SHAP to explain, but we use kernel explainer which is adequate for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_returns = testPredict.mean(axis=0)\n",
    "cov_matrix = np.cov(testPredict.T)\n",
    "num_portfolios = 10000\n",
    "\n",
    "results_frame = create_portfolios(pivot_df,num_portfolios,mean_daily_returns,cov_matrix)\n",
    "\n",
    "#Visualize \n",
    "plt.scatter(results_frame.stdev, results_frame.return, c=results_frame.sharpe, cmap='RdYlBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate position of portfolio with highest Sharpe Ratio\n",
    "max_sharpe_port = results_frame.iloc[results_frame['sharpe'].idxmax()]\n",
    "# Locate positon of portfolio with minimum standard deviation\n",
    "min_vol_port = results_frame.iloc[results_frame['stdev'].idxmin()]\n",
    "\n",
    "#Explain using SHAP\n",
    "explainer = shap.KernelExplainer(model.predict, trainX)\n",
    "# Compute SHAP values for a single prediction (e.g., first prediction)\n",
    "shap_values = explainer.shap_values(testX[0])\n",
    "# Plot the SHAP values\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], testX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fb21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3538a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
